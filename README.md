# RagReader

A AI-powered chatbot that uses Retrieval-Augmented Generation (RAG) to answer questions based on PDF documentation. Built with modern Python frameworks and designed for scalability and ease of use.

Note - The current status is ALPHA.

## Objective
The objective is to provide ready to use production grade RAG powered ChatBot.

### Use cases - 
1. Tech documenation once fed to chatbot can help onboard new developers.
2. Product Documentation once fed to chatbot can help onboard new product person help CSM, Clients with their queries.
3. Manual of any appliances can be fed and end users can ask any question and get answers easily.
4. The usecases are many and can be customised to make wonders.

## ğŸŒŸ Features

### Core Capabilities
- **ğŸ“„ Advanced PDF Processing**: 
  - Multi-pdf-library support (PyPDF2, pdfplumber, PyMuPDF)
  - Table extraction and formatting
  - Automatic text cleaning and preprocessing
  - Page-level metadata preservation

- **ğŸ”¤ Intelligent Text Chunking**:
  - Multiple chunking strategies (recursive, token-based, section-based)
  - Configurable chunk size and overlap
  - Context-aware splitting that preserves meaning
  - Neighbor chunk retrieval for enhanced context

- **ğŸ” Semantic Search**:
  - Vector similarity search using embeddings
  - Support for multiple embedding models
  - Relevance scoring and reranking
  - Metadata filtering capabilities

- **ğŸ¤– Multi-LLM Support**:
  - OpenAI (GPT-4, GPT-3.5)
  - Anthropic (Claude 3)
  - Local models via Ollama (todo - coming soon)
  - Easy switching between providers

- **ğŸ’¾ Vector Database Options**:
  - ChromaDB (default, local)
  - Pinecone - Todo Add support
  - FAISS - Todo Add support

### User Interfaces

1. **ğŸ–¥ï¸ Command Line Interface (CLI)**

2. **ğŸŒ Web Interface (React App)**

3. **ğŸš€ REST API (FastAPI)**

## ğŸ“‹ Prerequisites

- Python 3.11 or higher
- 4GB+ RAM recommended
- API keys for LLM providers (OpenAI/Anthropic)

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd chatbot-tech-rag

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys
# Required: OPENAI_API_KEY or ANTHROPIC_API_KEY
# Optional: PINECONE_API_KEY (if using Pinecone)
```

### 3. Add Documents

```bash
# Place PDF files in the train-data directory
cp /path/to/your/docs/*.pdf train-data/
```

### 4. Index Documents

```bash
# Run the indexing script
python scripts/index_documents.py

# With options
python scripts/index_documents.py --clear  # Clear existing index
python scripts/index_documents.py --config config/dev.yaml  --clear   --verify # Use specific config
python scripts/index_documents.py --verify  # Verify after indexing
```

### 5. Run the Chatbot

**Option A: Command Line Interface**
```bash
# Interactive mode
python -m src.main

# Single question mode
python -m src.main --config config/dev.yaml --question "what is XYZ?"

# With custom configuration
python -m src.main --config config/prod.yaml
```

## ğŸ”§ Configuration

### Environment Variables (.env)

```env
# LLM Configuration
OPENAI_API_KEY=your_openai_key_here
OPENAI_MODEL=gpt-4o-mini
ANTHROPIC_API_KEY=your_anthropic_key_here
ANTHROPIC_MODEL=claude-3-haiku-20240307

# Vector Store Configuration
VECTOR_STORE_TYPE=chromadb
CHROMA_PERSIST_DIRECTORY=./chroma_db
CHROMA_COLLECTION_NAME=tech_docs

# Embedding Configuration
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Chunking Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/chatbot.log
```

### YAML Configuration Files

The system supports YAML configuration files for different environments:

```yaml
# config/dev.yaml
embedding:
  type: sentence_transformer
  model_name: all-MiniLM-L6-v2

vector_store:
  type: chromadb
  persist_directory: ./chroma_db_dev

llm:
  provider: openai
  model: gpt-4o-mini
  temperature: 0.7

rag:
  top_k: 5
  rerank: true
  include_neighbors: true
```

## ğŸ“ Project Structure

```
chatbot-tech-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pdf_processor/     # PDF extraction and text processing
â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py    # Multi-backend PDF text extraction
â”‚   â”‚   â””â”€â”€ text_chunker.py     # Intelligent text chunking
â”‚   â”œâ”€â”€ embeddings/        # Embedding generation
â”‚   â”‚   â”œâ”€â”€ base.py            # Abstract base class
â”‚   â”‚   â”œâ”€â”€ sentence_transformer.py  # Local embeddings
â”‚   â”‚   â”œâ”€â”€ openai_embedding.py     # OpenAI embeddings
â”‚   â”‚   â””â”€â”€ factory.py         # Embedding model factory
â”‚   â”œâ”€â”€ vector_store/      # Vector database operations
â”‚   â”‚   â”œâ”€â”€ base.py           # Abstract base class
â”‚   â”‚   â”œâ”€â”€ chromadb_store.py # ChromaDB implementation
â”‚   â”‚   â””â”€â”€ factory.py        # Vector store factory
â”‚   â”œâ”€â”€ rag/              # RAG implementation
â”‚   â”‚   â”œâ”€â”€ retriever.py      # Document retrieval
â”‚   â”‚   â””â”€â”€ generator.py      # Answer generation
â”‚   â”œâ”€â”€ llm/              # LLM integrations
â”‚   â”‚   â”œâ”€â”€ base.py          # Abstract base class
â”‚   â”‚   â”œâ”€â”€ openai_client.py # OpenAI implementation
â”‚   â”‚   â”œâ”€â”€ anthropic_client.py # Anthropic implementation
â”‚   â”‚   â””â”€â”€ factory.py       # LLM client factory
â”‚   â”œâ”€â”€ cli/              # Command-line interface
â”‚   â”‚   â””â”€â”€ chatbot_cli.py   # Rich CLI implementation
â”‚   â”œâ”€â”€ api/              # REST API
â”‚   â”‚   â””â”€â”€ app.py          # FastAPI application
â”‚   â””â”€â”€ utils/            # Utilities
â”‚       â”œâ”€â”€ config.py       # Configuration management
â”‚       â””â”€â”€ logger.py       # Logging setup
â”œâ”€â”€ scripts/              # Utility scripts
â”‚   â””â”€â”€ index_documents.py  # Document indexing script
â”œâ”€â”€ config/               # Configuration files
â”‚   â”œâ”€â”€ dev.yaml           # Development config
â”‚   â””â”€â”€ prod.yaml          # Production config
â”œâ”€â”€ train-data/           # PDF documents directory
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example         # Environment template
â””â”€â”€ README.md            # This file
```

## Todo -

1. **Embedding Caching**: Cache frequently used embeddings
2. **Batch Processing**: Process multiple documents in parallel
3. **Async Operations**: Use async endpoints for better concurrency
4. **Model Selection**: Choose appropriate models for your use case
5. **Index Optimization**: Regularly optimize vector indices

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

##  Support
- Create an issue for bug reports or feature requests

---
